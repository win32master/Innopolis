{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec0577-f727-42ed-bae4-aef1dcb079d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import lxml.html as html\n",
    "import pymorphy2\n",
    "import string\n",
    "from dask.distributed import Client\n",
    "import dask.bag as db\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "url=r\"http://lit.lib.ru/d/drozd_t_p/text_0240-1.shtml\"\n",
    "\n",
    "####\n",
    "def pymorphy2_311_hotfix():\n",
    "    from inspect import getfullargspec\n",
    "    from pymorphy2.units.base import BaseAnalyzerUnit\n",
    "    def _get_param_names_311(klass):\n",
    "        if klass.__init__ is object.__init__:\n",
    "            return []\n",
    "        args = getfullargspec(klass.__init__).args\n",
    "        return sorted(args[1:])\n",
    "    setattr(BaseAnalyzerUnit, '_get_param_names', _get_param_names_311)\n",
    "\n",
    "def process_text(text):\n",
    "    pymorphy2_311_hotfix() # Вызов костыля\n",
    "    morph = pymorphy2.MorphAnalyzer() # Создание морфологического анализатора\n",
    "    words = text.lower().translate(str.maketrans('', '', string.punctuation)).translate(str.maketrans('', '', string.digits)).split() # Перевод в нижний регистр, очистка от знаков пунктуации и цифр.\n",
    "    lemmatized_words = [morph.parse(word)[0].normal_form for word in words] # Лемматизация\n",
    "    return [word for word in lemmatized_words if len(word) >= 3] # Оставляем слова не короче 3 символов\n",
    "####\n",
    "\n",
    "response=requests.get(url)\n",
    "status_code=response.status_code\n",
    "if status_code==200: # Проверяем код ответа\n",
    "    soup=bs4.BeautifulSoup(response.text, features='lxml')\n",
    "    text=soup.body.find('div') # Пропускаем header и footer\n",
    "    [e.clear() for e in text.find_all('i')] # Поиск и удаление курсивного текста (название произведения и глав)\n",
    "    text=text.get_text(' ', strip=True)  # Извлекаем оставшийся текст\n",
    "    client = Client(n_workers = 6, threads_per_worker = 2, processes = True, memory_limit = '2GB') # Создание клиента DASK\n",
    "    bag=db.from_sequence([text]) # Создаем объект bag\n",
    "    words=bag.map(process_text).flatten() # Получаем список слов \n",
    "    word_counts=words.frequencies() # Получаем список из кортежей слов с их частотами\n",
    "    df=pd.DataFrame(word_counts, columns=['Слово', 'Частота']).sort_values(by=\"Частота\", ascending=False).head(10) # Создаем отсортированный датафрейм и берем первые 10 значений\n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title (\"Наиболее часто встречающиеся слова\")\n",
    "    sns.barplot(data=df, x='Слово', y='Частота')\n",
    "    bars = plt.bar(df['Слово'], df['Частота'])\n",
    "    for bar in bars: # Добавление аннотаций\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{bar.get_height():.0f}', ha='center', va='bottom', fontsize=10)\n",
    "    plt.show()\n",
    "else:\n",
    "    print (f\"Возникла ошибка при загрузке текста. Код ошибки: {status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
